{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "VGG viz Filters colab GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXIrNiJL4vrq",
        "colab_type": "text"
      },
      "source": [
        "## VGG 16 viz Filters\n",
        "\n",
        "#### author info\n",
        "* by Aven Le ZHOU (https://www.aven.cc)\n",
        "* artMahcines & NYU Shanghai\n",
        "* aiarts, spring 2020\n",
        "* https://github.com/artmachines/aiarts2020\n",
        "\n",
        "#### credits\n",
        "* Original code implementation from keras team [link](https://keras.io/examples/conv_filter_visualization/)\n",
        "* adapted by Aven for aiarts 2020 course\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jsp0fci4vrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c1f8856-75d0-4bda-f7ba-84237a0d985c"
      },
      "source": [
        "# use_plaidML_backend = False\n",
        "# use_colab = True\n",
        "\n",
        "# import os\n",
        "# if use_plaidML_backend and (not use_colab): \n",
        "#     os.environ[\"KERAS_BACKEND\"]=\"plaidml.keras.backend\"\n",
        "# else:\n",
        "#   if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "#     print('ERROR: Not connected to a TPU runtime; runtime > change runtime type')\n",
        "#   else:\n",
        "#     tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#     print ('TPU address is', tpu_address)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.104.49.186:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bntvnehAAxy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "27c802fd-c9ea-46d1-ba41-73b31957fc3d"
      },
      "source": [
        "use_plaidML_backend = False\n",
        "use_colab = True\n",
        "\n",
        "if use_plaidML_backend and (not use_colab): \n",
        "    import os\n",
        "    os.environ[\"KERAS_BACKEND\"]=\"plaidml.keras.backend\"\n",
        "else:\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "    print('and then re-execute this cell.')\n",
        "  else:\n",
        "    print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar 20 17:35:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_BiJ0YHNs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "051cd1ce-780d-4e19-bda8-960249f2b880"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvyicSnH4vry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "313469ba-8591-4eea-891b-f5cf959daf0f"
      },
      "source": [
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras.applications import vgg16\n",
        "from PIL import Image as pil_image\n",
        "\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJH_3cNw4vr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x): return (x / (K.sqrt(K.mean(K.square(x))) + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veXtDZAC4vr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_image(x):\n",
        "    \"\"\"convert a float array into a valid uint8 image. x: A numpy-array representing the generated image.\"\"\"\n",
        "    # normalize tensor: center on 0., ensure std is 0.25\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + K.epsilon())\n",
        "    x *= 0.25\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'channels_first': x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stsPBCT54vr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_image(x, former):\n",
        "    \"\"\"convert a valid uint8 image back into a float array. Reverses `decode_image`.\"\"\"\n",
        "    if K.image_data_format() == 'channels_first': x = x.transpose((2, 0, 1))\n",
        "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQdiOYCm4vr_",
        "colab_type": "text"
      },
      "source": [
        "### visualize_layer: visualizes the most relevant filters of one conv-layer in a certain model.\n",
        "* model: The model containing layer_name.\n",
        "* layer_name: The name of the layer to be visualized. Has to be a part of model.\n",
        "* step: step size for gradient ascent.\n",
        "* epochs: Number of iterations for gradient ascent.\n",
        "* upscaling_steps: Number of upscaling steps. Starting image is in this case (80, 80).\n",
        "* upscaling_factor: Factor to which to slowly upgrade the image towards output_dim.\n",
        "* output_dim: [img_width, img_height] The output image dimensions.\n",
        "* filter_range: Tupel[lower, upper]\n",
        "              Determines the to be computed filter numbers.\n",
        "              If the second value is `None`,\n",
        "              the last filter will be inferred as the upper boundary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUeb5jxZ4vsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_layer(model,\n",
        "                    layer_name,\n",
        "                    step=1.,\n",
        "                    epochs=15,\n",
        "                    upscaling_steps=9,\n",
        "                    upscaling_factor=1.2,\n",
        "                    output_dim=(412, 412),\n",
        "                    filter_range=(0, None)):\n",
        "\n",
        "\n",
        "    def _generate_filter_image(input_img,\n",
        "                               layer_output,\n",
        "                               filter_index):\n",
        "        \"\"\"\n",
        "            Generates image for one particular filter.\n",
        "            input_img: The input-image Tensor.\n",
        "            layer_output: The output-image Tensor.\n",
        "            filter_index: The to be processed filter number. Assumed to be valid.\n",
        "            Returns: a tuple of the image (array) itself and the last loss. or None if no image could be generated.\n",
        "        \"\"\"\n",
        "        s_time = time.time()\n",
        "\n",
        "        # we build a loss function that maximizes the activation of the nth filter of the layer considered\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
        "        else:\n",
        "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # we compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, input_img)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads = normalize(grads)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([input_img], [loss, grads])\n",
        "\n",
        "        # we start from a gray image with some random noise\n",
        "        intermediate_dim = tuple(\n",
        "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            input_img_data = np.random.random(\n",
        "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
        "        else:\n",
        "            input_img_data = np.random.random(\n",
        "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
        "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
        "\n",
        "        # Slowly upscaling towards the original size prevents\n",
        "        # a dominating high-frequency of the to visualized structure\n",
        "        # as it would occur if we directly compute the 412d-image.\n",
        "        # Behaves as a better starting point for each following dimension\n",
        "        # and therefore avoids poor local minima\n",
        "        for up in reversed(range(upscaling_steps)):\n",
        "            # we run gradient ascent for e.g. 20 steps\n",
        "            for _ in range(epochs):\n",
        "                loss_value, grads_value = iterate([input_img_data])\n",
        "                input_img_data += grads_value * step\n",
        "\n",
        "                # some filters get stuck to 0, we can skip them\n",
        "                if loss_value <= K.epsilon():\n",
        "                    return None\n",
        "\n",
        "            # Calculate upscaled dimension\n",
        "            intermediate_dim = tuple(\n",
        "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
        "            # Upscale\n",
        "            img = decode_image(input_img_data[0])\n",
        "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
        "                                                           pil_image.BICUBIC))\n",
        "            input_img_data = np.expand_dims(\n",
        "                encode_image(img, input_img_data[0]), 0)\n",
        "\n",
        "        # decode the resulting input image\n",
        "        img = decode_image(input_img_data[0])\n",
        "        e_time = time.time()\n",
        "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
        "                                                                  loss_value,\n",
        "                                                                  e_time - s_time))\n",
        "        return img, loss_value\n",
        "\n",
        "    def _draw_filters(filters, n=None):\n",
        "        \"\"\"Draw the best filters in a nxn grid.\n",
        "\n",
        "        # Arguments\n",
        "            filters: A List of generated images and their corresponding losses\n",
        "                     for each processed filter.\n",
        "            n: dimension of the grid.\n",
        "               If none, the largest possible square will be used\n",
        "        \"\"\"\n",
        "        if n is None:\n",
        "            n = int(np.floor(np.sqrt(len(filters))))\n",
        "\n",
        "        # the filters that have the highest loss are assumed to be better-looking.\n",
        "        # we will only keep the top n*n filters.\n",
        "        filters.sort(key=lambda x: x[1], reverse=True)\n",
        "        filters = filters[:n * n]\n",
        "\n",
        "        # build a black picture with enough space for\n",
        "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
        "        MARGIN = 5\n",
        "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
        "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
        "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
        "\n",
        "        # fill the picture with our saved filters\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                img, _ = filters[i * n + j]\n",
        "                width_margin = (output_dim[0] + MARGIN) * i\n",
        "                height_margin = (output_dim[1] + MARGIN) * j\n",
        "                stitched_filters[\n",
        "                    width_margin: width_margin + output_dim[0],\n",
        "                    height_margin: height_margin + output_dim[1],\n",
        "                    :] = img\n",
        "\n",
        "        # save the result to disk\n",
        "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
        "\n",
        "    # this is the placeholder for the input images\n",
        "    assert len(model.inputs) == 1\n",
        "    input_img = model.inputs[0]\n",
        "\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "\n",
        "    output_layer = layer_dict[layer_name]\n",
        "    assert isinstance(output_layer, layers.Conv2D)\n",
        "\n",
        "    # Compute to be processed filter range\n",
        "    filter_lower = filter_range[0]\n",
        "    filter_upper = (filter_range[1]\n",
        "                    if filter_range[1] is not None\n",
        "                    else len(output_layer.get_weights()[1]))\n",
        "    assert(filter_lower >= 0\n",
        "           and filter_upper <= len(output_layer.get_weights()[1])\n",
        "           and filter_upper > filter_lower)\n",
        "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
        "\n",
        "    # iterate through each filter and generate its corresponding image\n",
        "    processed_filters = []\n",
        "    for f in range(filter_lower, filter_upper):\n",
        "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
        "\n",
        "        if img_loss is not None:\n",
        "            processed_filters.append(img_loss)\n",
        "\n",
        "    print('{} filter processed.'.format(len(processed_filters)))\n",
        "    # Finally draw and store the best filters to disk\n",
        "    _draw_filters(processed_filters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz1b86sD4vsE",
        "colab_type": "code",
        "outputId": "20f850d9-4138-4831-b43b-ba47017baaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # the name of the layer we want to visualize\n",
        "    # (see model definition at keras/applications/vgg16.py)\n",
        "    LAYER_NAME = 'block4_conv3'\n",
        "\n",
        "    # build the VGG16 network with ImageNet weights\n",
        "    vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n",
        "    print('Model loaded.')\n",
        "    vgg.summary()\n",
        "\n",
        "    # example function call\n",
        "    visualize_layer(vgg, LAYER_NAME)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Model loaded.\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Compute filters 0 to 512\n",
            "Costs of filter   2:   685 ( 3.30s )\n",
            "Costs of filter   4:   673 ( 1.35s )\n",
            "Costs of filter   5:   411 ( 1.34s )\n",
            "Costs of filter   6:   431 ( 1.47s )\n",
            "Costs of filter   7:   525 ( 1.36s )\n",
            "Costs of filter   8:   317 ( 1.36s )\n",
            "Costs of filter   9:   285 ( 1.39s )\n",
            "Costs of filter  11:   407 ( 1.40s )\n",
            "Costs of filter  13:   656 ( 1.38s )\n",
            "Costs of filter  14:   828 ( 1.40s )\n",
            "Costs of filter  15:   365 ( 1.38s )\n",
            "Costs of filter  16:   599 ( 1.41s )\n",
            "Costs of filter  17:   391 ( 1.38s )\n",
            "Costs of filter  19:   442 ( 1.40s )\n",
            "Costs of filter  20:   525 ( 1.44s )\n",
            "Costs of filter  21:   528 ( 1.43s )\n",
            "Costs of filter  22:   443 ( 1.41s )\n",
            "Costs of filter  24:   596 ( 1.42s )\n",
            "Costs of filter  25:   821 ( 1.42s )\n",
            "Costs of filter  26:   408 ( 1.41s )\n",
            "Costs of filter  27:   553 ( 1.43s )\n",
            "Costs of filter  28:   703 ( 1.44s )\n",
            "Costs of filter  29:   415 ( 1.42s )\n",
            "Costs of filter  30:   549 ( 1.43s )\n",
            "Costs of filter  32:   738 ( 1.45s )\n",
            "Costs of filter  35:   460 ( 1.43s )\n",
            "Costs of filter  39:   485 ( 1.48s )\n",
            "Costs of filter  40:   638 ( 1.52s )\n",
            "Costs of filter  42:   608 ( 1.51s )\n",
            "Costs of filter  45:   558 ( 1.54s )\n",
            "Costs of filter  47:   831 ( 1.52s )\n",
            "Costs of filter  48:   534 ( 1.53s )\n",
            "Costs of filter  52:  1515 ( 1.56s )\n",
            "Costs of filter  55:   497 ( 1.55s )\n",
            "Costs of filter  57:   407 ( 1.58s )\n",
            "Costs of filter  58:   233 ( 1.59s )\n",
            "Costs of filter  59:   570 ( 1.55s )\n",
            "Costs of filter  60:   677 ( 1.62s )\n",
            "Costs of filter  61:  1066 ( 1.61s )\n",
            "Costs of filter  64:   482 ( 1.60s )\n",
            "Costs of filter  66:   634 ( 1.61s )\n",
            "Costs of filter  67:   431 ( 1.59s )\n",
            "Costs of filter  68:   375 ( 1.70s )\n",
            "Costs of filter  69:   506 ( 1.62s )\n",
            "Costs of filter  71:   752 ( 1.65s )\n",
            "Costs of filter  73:   486 ( 1.62s )\n",
            "Costs of filter  78:   494 ( 1.66s )\n",
            "Costs of filter  79:   654 ( 1.70s )\n",
            "Costs of filter  81:   375 ( 1.69s )\n",
            "Costs of filter  82:   691 ( 1.68s )\n",
            "Costs of filter  84:   597 ( 1.71s )\n",
            "Costs of filter  86:  1232 ( 1.72s )\n",
            "Costs of filter  87:   668 ( 1.69s )\n",
            "Costs of filter  88:   631 ( 1.68s )\n",
            "Costs of filter  89:   706 ( 1.74s )\n",
            "Costs of filter  91:   960 ( 1.78s )\n",
            "Costs of filter  97:   963 ( 1.77s )\n",
            "Costs of filter  99:   687 ( 1.81s )\n",
            "Costs of filter 100:   595 ( 1.80s )\n",
            "Costs of filter 101:   606 ( 1.80s )\n",
            "Costs of filter 102:   369 ( 1.81s )\n",
            "Costs of filter 104:   639 ( 1.76s )\n",
            "Costs of filter 105:   803 ( 1.80s )\n",
            "Costs of filter 106:   992 ( 1.78s )\n",
            "Costs of filter 111:   442 ( 1.86s )\n",
            "Costs of filter 112:   405 ( 1.87s )\n",
            "Costs of filter 113:   868 ( 1.85s )\n",
            "Costs of filter 116:   597 ( 1.87s )\n",
            "Costs of filter 117:   651 ( 1.89s )\n",
            "Costs of filter 118:   758 ( 1.88s )\n",
            "Costs of filter 121:   900 ( 1.92s )\n",
            "Costs of filter 129:   389 ( 1.95s )\n",
            "Costs of filter 130:   405 ( 1.93s )\n",
            "Costs of filter 132:   779 ( 1.95s )\n",
            "Costs of filter 134:   765 ( 1.97s )\n",
            "Costs of filter 137:   833 ( 1.99s )\n",
            "Costs of filter 138:   583 ( 2.03s )\n",
            "Costs of filter 139:   833 ( 2.03s )\n",
            "Costs of filter 141:   501 ( 2.03s )\n",
            "Costs of filter 142:   538 ( 2.02s )\n",
            "Costs of filter 143:   590 ( 2.03s )\n",
            "Costs of filter 144:   647 ( 2.07s )\n",
            "Costs of filter 146:   367 ( 2.05s )\n",
            "Costs of filter 149:   367 ( 2.10s )\n",
            "Costs of filter 153:   425 ( 2.15s )\n",
            "Costs of filter 154:   374 ( 2.17s )\n",
            "Costs of filter 155:   888 ( 2.13s )\n",
            "Costs of filter 156:   536 ( 2.10s )\n",
            "Costs of filter 158:   520 ( 2.14s )\n",
            "Costs of filter 160:   788 ( 2.15s )\n",
            "Costs of filter 162:   552 ( 2.15s )\n",
            "Costs of filter 163:   806 ( 2.15s )\n",
            "Costs of filter 166:   547 ( 2.16s )\n",
            "Costs of filter 167:   427 ( 2.22s )\n",
            "Costs of filter 169:   407 ( 2.12s )\n",
            "Costs of filter 173:   597 ( 2.23s )\n",
            "Costs of filter 175:   650 ( 2.18s )\n",
            "Costs of filter 179:   453 ( 2.27s )\n",
            "Costs of filter 181:   397 ( 2.26s )\n",
            "Costs of filter 183:   370 ( 2.27s )\n",
            "Costs of filter 185:   139 ( 2.31s )\n",
            "Costs of filter 187:   369 ( 2.30s )\n",
            "Costs of filter 188:   497 ( 2.36s )\n",
            "Costs of filter 192:   655 ( 2.33s )\n",
            "Costs of filter 195:   770 ( 2.31s )\n",
            "Costs of filter 197:   435 ( 2.41s )\n",
            "Costs of filter 199:   817 ( 2.34s )\n",
            "Costs of filter 201:   426 ( 2.34s )\n",
            "Costs of filter 202:   344 ( 2.40s )\n",
            "Costs of filter 209:   523 ( 2.44s )\n",
            "Costs of filter 210:   613 ( 2.46s )\n",
            "Costs of filter 212:   474 ( 2.41s )\n",
            "Costs of filter 213:   288 ( 2.47s )\n",
            "Costs of filter 214:   417 ( 2.45s )\n",
            "Costs of filter 217:   751 ( 2.43s )\n",
            "Costs of filter 218:   256 ( 2.44s )\n",
            "Costs of filter 219:   488 ( 2.47s )\n",
            "Costs of filter 227:   740 ( 2.51s )\n",
            "Costs of filter 229:   851 ( 2.53s )\n",
            "Costs of filter 230:   382 ( 2.52s )\n",
            "Costs of filter 235:  1067 ( 2.55s )\n",
            "Costs of filter 236:   674 ( 2.55s )\n",
            "Costs of filter 237:   586 ( 2.57s )\n",
            "Costs of filter 238:   480 ( 2.60s )\n",
            "Costs of filter 240:   513 ( 2.67s )\n",
            "Costs of filter 241:   475 ( 2.61s )\n",
            "Costs of filter 242:   554 ( 2.60s )\n",
            "Costs of filter 246:   635 ( 2.59s )\n",
            "Costs of filter 248:   700 ( 2.63s )\n",
            "Costs of filter 250:   614 ( 2.67s )\n",
            "Costs of filter 251:   729 ( 2.66s )\n",
            "Costs of filter 254:   465 ( 2.64s )\n",
            "Costs of filter 255:   378 ( 2.71s )\n",
            "Costs of filter 256:   621 ( 2.77s )\n",
            "Costs of filter 258:   556 ( 2.74s )\n",
            "Costs of filter 259:   854 ( 2.70s )\n",
            "Costs of filter 261:   523 ( 2.74s )\n",
            "Costs of filter 264:   599 ( 2.75s )\n",
            "Costs of filter 265:   894 ( 2.84s )\n",
            "Costs of filter 266:   527 ( 2.75s )\n",
            "Costs of filter 267:  1064 ( 2.76s )\n",
            "Costs of filter 268:   466 ( 2.79s )\n",
            "Costs of filter 273:   518 ( 2.78s )\n",
            "Costs of filter 275:   626 ( 2.83s )\n",
            "Costs of filter 280:   699 ( 2.95s )\n",
            "Costs of filter 284:   993 ( 3.03s )\n",
            "Costs of filter 285:   574 ( 3.11s )\n",
            "Costs of filter 286:   598 ( 3.08s )\n",
            "Costs of filter 288:   387 ( 2.99s )\n",
            "Costs of filter 289:   441 ( 2.99s )\n",
            "Costs of filter 291:   673 ( 3.01s )\n",
            "Costs of filter 292:   597 ( 3.24s )\n",
            "Costs of filter 293:   454 ( 3.37s )\n",
            "Costs of filter 294:   313 ( 3.22s )\n",
            "Costs of filter 296:   688 ( 3.24s )\n",
            "Costs of filter 297:   399 ( 3.21s )\n",
            "Costs of filter 298:   289 ( 3.13s )\n",
            "Costs of filter 301:   263 ( 3.32s )\n",
            "Costs of filter 305:   548 ( 3.29s )\n",
            "Costs of filter 307:   536 ( 3.39s )\n",
            "Costs of filter 308:  1126 ( 3.42s )\n",
            "Costs of filter 310:  1069 ( 3.36s )\n",
            "Costs of filter 311:   414 ( 3.40s )\n",
            "Costs of filter 312:   541 ( 3.39s )\n",
            "Costs of filter 313:   722 ( 3.41s )\n",
            "Costs of filter 315:   652 ( 3.39s )\n",
            "Costs of filter 317:   453 ( 3.45s )\n",
            "Costs of filter 318:   507 ( 3.44s )\n",
            "Costs of filter 319:   551 ( 3.42s )\n",
            "Costs of filter 322:   872 ( 3.41s )\n",
            "Costs of filter 324:   317 ( 3.39s )\n",
            "Costs of filter 325:   751 ( 3.47s )\n",
            "Costs of filter 326:   393 ( 3.43s )\n",
            "Costs of filter 327:   694 ( 3.51s )\n",
            "Costs of filter 329:   527 ( 3.31s )\n",
            "Costs of filter 332:   508 ( 3.32s )\n",
            "Costs of filter 333:   766 ( 3.42s )\n",
            "Costs of filter 336:   581 ( 3.40s )\n",
            "Costs of filter 340:   509 ( 3.40s )\n",
            "Costs of filter 342:   507 ( 3.39s )\n",
            "Costs of filter 345:   680 ( 3.45s )\n",
            "Costs of filter 346:   764 ( 3.45s )\n",
            "Costs of filter 348:   577 ( 3.53s )\n",
            "Costs of filter 351:   363 ( 3.61s )\n",
            "Costs of filter 352:   460 ( 3.61s )\n",
            "Costs of filter 355:   545 ( 3.55s )\n",
            "Costs of filter 356:    73 ( 3.54s )\n",
            "Costs of filter 358:   351 ( 3.49s )\n",
            "Costs of filter 360:  1124 ( 3.51s )\n",
            "Costs of filter 362:   469 ( 3.53s )\n",
            "Costs of filter 365:   807 ( 3.57s )\n",
            "Costs of filter 366:   784 ( 3.61s )\n",
            "Costs of filter 368:   579 ( 3.59s )\n",
            "Costs of filter 370:   707 ( 3.61s )\n",
            "Costs of filter 373:   723 ( 3.58s )\n",
            "Costs of filter 375:   598 ( 3.71s )\n",
            "Costs of filter 376:  1155 ( 3.65s )\n",
            "Costs of filter 377:   593 ( 3.67s )\n",
            "Costs of filter 379:   424 ( 3.67s )\n",
            "Costs of filter 381:   936 ( 3.80s )\n",
            "Costs of filter 382:   650 ( 3.69s )\n",
            "Costs of filter 384:   783 ( 3.74s )\n",
            "Costs of filter 389:   425 ( 3.74s )\n",
            "Costs of filter 391:   418 ( 3.82s )\n",
            "Costs of filter 393:   435 ( 3.77s )\n",
            "Costs of filter 394:   758 ( 3.83s )\n",
            "Costs of filter 395:   541 ( 3.84s )\n",
            "Costs of filter 396:   608 ( 3.87s )\n",
            "Costs of filter 397:   702 ( 3.83s )\n",
            "Costs of filter 399:   487 ( 3.78s )\n",
            "Costs of filter 400:   930 ( 3.87s )\n",
            "Costs of filter 401:   332 ( 3.80s )\n",
            "Costs of filter 404:   370 ( 3.78s )\n",
            "Costs of filter 408:   616 ( 3.85s )\n",
            "Costs of filter 410:   437 ( 3.90s )\n",
            "Costs of filter 413:   739 ( 3.86s )\n",
            "Costs of filter 414:   271 ( 3.92s )\n",
            "Costs of filter 418:   871 ( 3.98s )\n",
            "Costs of filter 423:   209 ( 3.98s )\n",
            "Costs of filter 425:   254 ( 4.11s )\n",
            "Costs of filter 430:   640 ( 4.06s )\n",
            "Costs of filter 431:   656 ( 4.12s )\n",
            "Costs of filter 434:   532 ( 4.13s )\n",
            "Costs of filter 436:   635 ( 4.09s )\n",
            "Costs of filter 437:   510 ( 4.12s )\n",
            "Costs of filter 439:   625 ( 4.18s )\n",
            "Costs of filter 441:   813 ( 4.15s )\n",
            "Costs of filter 443:   783 ( 4.19s )\n",
            "Costs of filter 444:   847 ( 4.17s )\n",
            "Costs of filter 448:   394 ( 4.21s )\n",
            "Costs of filter 450:   889 ( 4.35s )\n",
            "Costs of filter 456:   385 ( 4.33s )\n",
            "Costs of filter 457:   185 ( 4.39s )\n",
            "Costs of filter 458:   573 ( 4.46s )\n",
            "Costs of filter 460:   151 ( 4.37s )\n",
            "Costs of filter 462:   352 ( 4.65s )\n",
            "Costs of filter 463:  1099 ( 4.42s )\n",
            "Costs of filter 464:   620 ( 4.49s )\n",
            "Costs of filter 466:   474 ( 4.50s )\n",
            "Costs of filter 467:   405 ( 4.52s )\n",
            "Costs of filter 468:   577 ( 4.43s )\n",
            "Costs of filter 470:   379 ( 4.55s )\n",
            "Costs of filter 476:   871 ( 4.52s )\n",
            "Costs of filter 477:   505 ( 4.65s )\n",
            "Costs of filter 479:   990 ( 4.57s )\n",
            "Costs of filter 480:   682 ( 4.57s )\n",
            "Costs of filter 481:   421 ( 4.65s )\n",
            "Costs of filter 483:   632 ( 4.60s )\n",
            "Costs of filter 484:   333 ( 4.70s )\n",
            "Costs of filter 485:   857 ( 4.64s )\n",
            "Costs of filter 486:   659 ( 4.58s )\n",
            "Costs of filter 487:   438 ( 4.62s )\n",
            "Costs of filter 489:   416 ( 4.59s )\n",
            "Costs of filter 491:  1023 ( 4.69s )\n",
            "Costs of filter 493:   552 ( 4.68s )\n",
            "Costs of filter 494:   705 ( 4.75s )\n",
            "Costs of filter 495:   397 ( 4.71s )\n",
            "Costs of filter 496:   480 ( 4.66s )\n",
            "Costs of filter 497:   551 ( 4.66s )\n",
            "Costs of filter 498:   603 ( 4.73s )\n",
            "Costs of filter 499:   436 ( 4.66s )\n",
            "Costs of filter 500:   556 ( 4.66s )\n",
            "Costs of filter 502:   872 ( 4.74s )\n",
            "Costs of filter 504:   479 ( 4.68s )\n",
            "Costs of filter 507:   935 ( 4.69s )\n",
            "Costs of filter 509:  1330 ( 4.68s )\n",
            "Costs of filter 510:   769 ( 4.77s )\n",
            "267 filter processed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}